{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Keras/TF\n",
    "from sklearn.svm import SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "\n",
    "#SKLearn\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#NLTK Functions\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LogLoss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../merged_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "data['title_tokenized'] = [word_tokenize(i) for i in data['Headline']]\n",
    "\n",
    "filtered = []\n",
    "for words in data['title_tokenized']:\n",
    "    temp = []\n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            temp.append(w)\n",
    "    filtered.append(temp)\n",
    "\n",
    "data['title_no_stops'] = filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Compound Score</th>\n",
       "      <th>Read/Fake</th>\n",
       "      <th>Character Count</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Upper Characters</th>\n",
       "      <th>Lower Case Characters</th>\n",
       "      <th>SpecialChar Count</th>\n",
       "      <th>title_tokenized</th>\n",
       "      <th>title_no_stops</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>#2816: Clinton Pride’s 8(a) Pig Farm Bridge – ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>fake</td>\n",
       "      <td>97</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>56</td>\n",
       "      <td>8</td>\n",
       "      <td>[#, 2816, :, Clinton, Pride, ’, s, 8, (, a, ),...</td>\n",
       "      <td>[#, 2816, :, Clinton, Pride, ’, 8, (, ), Pig, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>#2817: Serco's Zulu Starnet Blackmail – Clinto...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>fake</td>\n",
       "      <td>88</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>51</td>\n",
       "      <td>7</td>\n",
       "      <td>[#, 2817, :, Serco, 's, Zulu, Starnet, Blackma...</td>\n",
       "      <td>[#, 2817, :, Serco, 's, Zulu, Starnet, Blackma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Roger Stone update on Stop the Steal exit poll...</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.713</td>\n",
       "      <td>-0.9313</td>\n",
       "      <td>fake</td>\n",
       "      <td>456</td>\n",
       "      <td>72</td>\n",
       "      <td>14</td>\n",
       "      <td>358</td>\n",
       "      <td>13</td>\n",
       "      <td>[Roger, Stone, update, on, Stop, the, Steal, e...</td>\n",
       "      <td>[Roger, Stone, update, Stop, Steal, exit, poll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>#2818: Serco's Zulu Bridge To Mumbai Pig Farm ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>fake</td>\n",
       "      <td>91</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "      <td>[#, 2818, :, Serco, 's, Zulu, Bridge, To, Mumb...</td>\n",
       "      <td>[#, 2818, :, Serco, 's, Zulu, Bridge, To, Mumb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Trump Advocates the American People's Control ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>fake</td>\n",
       "      <td>66</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "      <td>[Trump, Advocates, the, American, People, 's, ...</td>\n",
       "      <td>[Trump, Advocates, American, People, 's, Contr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  \\\n",
       "0           0             0   \n",
       "1           1             1   \n",
       "2           2             2   \n",
       "3           3             3   \n",
       "4           4             4   \n",
       "\n",
       "                                            Headline  Negative  Positive  \\\n",
       "0  #2816: Clinton Pride’s 8(a) Pig Farm Bridge – ...     0.000      0.00   \n",
       "1  #2817: Serco's Zulu Starnet Blackmail – Clinto...     0.000      0.00   \n",
       "2  Roger Stone update on Stop the Steal exit poll...     0.237      0.05   \n",
       "3  #2818: Serco's Zulu Bridge To Mumbai Pig Farm ...     0.000      0.00   \n",
       "4  Trump Advocates the American People's Control ...     0.000      0.00   \n",
       "\n",
       "   Neutral  Compound Score Read/Fake  Character Count  Word Count  \\\n",
       "0    1.000          0.0000      fake               97          16   \n",
       "1    1.000          0.0000      fake               88          15   \n",
       "2    0.713         -0.9313      fake              456          72   \n",
       "3    1.000          0.0000      fake               91          17   \n",
       "4    1.000          0.0000      fake               66           9   \n",
       "\n",
       "   Upper Characters  Lower Case Characters  SpecialChar Count  \\\n",
       "0                13                     56                  8   \n",
       "1                11                     51                  7   \n",
       "2                14                    358                 13   \n",
       "3                12                     47                  8   \n",
       "4                 9                     46                  3   \n",
       "\n",
       "                                     title_tokenized  \\\n",
       "0  [#, 2816, :, Clinton, Pride, ’, s, 8, (, a, ),...   \n",
       "1  [#, 2817, :, Serco, 's, Zulu, Starnet, Blackma...   \n",
       "2  [Roger, Stone, update, on, Stop, the, Steal, e...   \n",
       "3  [#, 2818, :, Serco, 's, Zulu, Bridge, To, Mumb...   \n",
       "4  [Trump, Advocates, the, American, People, 's, ...   \n",
       "\n",
       "                                      title_no_stops  \n",
       "0  [#, 2816, :, Clinton, Pride, ’, 8, (, ), Pig, ...  \n",
       "1  [#, 2817, :, Serco, 's, Zulu, Starnet, Blackma...  \n",
       "2  [Roger, Stone, update, Stop, Steal, exit, poll...  \n",
       "3  [#, 2818, :, Serco, 's, Zulu, Bridge, To, Mumb...  \n",
       "4  [Trump, Advocates, American, People, 's, Contr...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode y's and train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(data['Read/Fake'].values)\n",
    "X = data['Headline'].values\n",
    "X_feat = data[['Negative','Positive','Neutral','Character Count','Word Count','Upper Characters','Lower Case Characters','SpecialChar Count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfeattrain, xfeatvalid, ytrain, yvalid = train_test_split(X_feat, y, stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use OOTB Vectorizer Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfv =  tfv.transform(xtrain) \n",
    "xvalid_tfv = tfv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Function Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.355 \n",
      "[[1331  330]\n",
      " [ 216 2102]]\n",
      "Score: 0.8627795928625283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on TF-IDF\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "predictions_y = clf.predict(xvalid_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "print (confusion_matrix(yvalid,predictions_y))\n",
    "print (f'Score: {clf.score(xvalid_tfv,yvalid)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.563 \n",
      "[[ 856  805]\n",
      " [ 210 2108]]\n",
      "Score: 0.744910781603418\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on Counts\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xfeattrain, ytrain)\n",
    "predictions = clf.predict_proba(xfeatvalid)\n",
    "predictions_y = clf.predict(xfeatvalid)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "print (confusion_matrix(yvalid,predictions_y))\n",
    "print (f'Score: {clf.score(xfeatvalid,yvalid)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.329 \n",
      "[[1324  337]\n",
      " [ 195 2123]]\n",
      "Score: 0.8662980648404122\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Naive Bayes on TFIDF\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "predictions_y = clf.predict(xvalid_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "print (confusion_matrix(yvalid,predictions_y))\n",
    "print (f'Score: {clf.score(xvalid_tfv,yvalid)}')\n",
    "# print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.708 \n",
      "[[ 907  754]\n",
      " [ 443 1875]]\n",
      "Score: 0.6991706458909274\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Naive Bayes on TFIDF\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xfeattrain, ytrain)\n",
    "predictions = clf.predict_proba(xfeatvalid)\n",
    "predictions_y = clf.predict(xfeatvalid)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "print (confusion_matrix(yvalid,predictions_y))\n",
    "print (f'Score: {clf.score(xfeatvalid,yvalid)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XG BOOOOOOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.459 \n",
      "[[ 975  686]\n",
      " [ 161 2157]]\n",
      "Score: 0.7871324453380246\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on tf-idf\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_tfv.tocsc(), ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv.tocsc())\n",
    "predictions_y = clf.predict(xvalid_tfv.tocsc())\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "print (confusion_matrix(yvalid,predictions_y))\n",
    "print (f'Score: {clf.score(xvalid_tfv,yvalid)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.418 \n",
      "[[1122  539]\n",
      " [ 211 2107]]\n",
      "Score: 0.8115104297562201\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on ctv\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xfeattrain, ytrain)\n",
    "predictions = clf.predict_proba(xfeatvalid)\n",
    "predictions_y = clf.predict(xfeatvalid)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "print (confusion_matrix(yvalid,predictions_y))\n",
    "print (f'Score: {clf.score(xfeatvalid,yvalid)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
